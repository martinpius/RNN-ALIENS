{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence to sequence model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNTQGcE/PnTSxv5kx6v+S9v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/RNN-ALIENS/blob/main/Sequence_to_sequence_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "KgovuPAxDL3w",
        "outputId": "e2868f15-1ca6-497d-903e-6a192bbfed6f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import tensorflow as tf\n",
        "  print(f\"You are using Colab with tensorflow version {tf.__version__}\")\n",
        "except Exception as e:\n",
        "  COLAB = False\n",
        "  print(f\"{type(e)}: {e}\\n....Please Load Your Drive....\")\n",
        "\n",
        "def time_fmt(x):\n",
        "  h = int(x / (60 * 60))\n",
        "  m = int(x % (60 * 60) / 60)\n",
        "  s = int(x % 60)\n",
        "  return f\"{h}: {m:>03}: {s:>05.2f}\"\n",
        "\n",
        "time_fmt(240.892)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "You are using Colab with tensorflow version 2.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0: 004: 00.00'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMwrOKsREtDn"
      },
      "source": [
        "import time, io, os, re, unicodedata\n",
        "import matplotlib as mlp\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.ticker as ticker\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbZGLDLnFgwR"
      },
      "source": [
        "#Lets build an encoder-decoder network for machine translation"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDu5C5pfFtKG"
      },
      "source": [
        "#Importing and preprocessing the data\n",
        "#We will train a machine to translate spanish language to english\n",
        "#It is a simple MT with attention mechanism to learn variable/words contribution"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utqRIxdjF5vr"
      },
      "source": [
        "folder_path = tf.keras.utils.get_file(fname = \"spa-eng.zip\", origin = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "                                      extract = True)\n",
        "file_path = os.path.dirname(folder_path) + \"/spa-eng/spa.txt\""
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Yiq0qiHyNH"
      },
      "source": [
        "#Change the unicode data format into ascii format:\n",
        "def ascii_fmt(t):\n",
        "  return \"\".join(k for k in unicodedata.normalize('NFD',t) if unicodedata.category(k) != 'Mn')"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR7BVnjGIrPg"
      },
      "source": [
        "def preprocess_text(t):\n",
        "  '''We do some cleaning and marking the start and the end of each sentence'''\n",
        "  t = ascii_fmt(t.lower().strip()) #Convert to lower cases and strip the white spaces\n",
        "  t = re.sub(r\"([?,!.多])\", r\" \\1 \", t)\n",
        "  t = re.sub(r'[\" \"]+', \" \", t)\n",
        "  t = re.sub(r'[^a-zA-Z?多,.!]+',\" \", t) # For each sentence we replace the everthing else except the one listed with a white space\n",
        "  t = t.strip() #Strip the white spaces\n",
        "  t = '<start>' + t + '<end>' #Marking the start and the end of each sentence with start, end\n",
        "  return t"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHWwHWiEI_Pz"
      },
      "source": [
        "#Testing the above function if it works as intended:\n",
        "en_verse = u\"May I borrow this book?\"\n",
        "sp_verse = u\"多Puedo tomar prestado este libro?\""
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O80oUj8ZJOJh"
      },
      "source": [
        "en_out = preprocess_text(en_verse)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oda7vrpQJV3T"
      },
      "source": [
        "sp_out = preprocess_text(sp_verse)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu2ja5tPJaZs",
        "outputId": "5e9f8680-2c9c-426e-ea6f-2dce9b319641"
      },
      "source": [
        "print(en_out)\n",
        "print()\n",
        "print(sp_out)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start>may i borrow this book ?<end>\n",
            "\n",
            "<start>多 puedo tomar prestado este libro ?<end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqif5h7WJg0I"
      },
      "source": [
        "#Create sentences pairs for english-spanish side by side"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7v5s_DjJyAo"
      },
      "source": [
        "def data_creator(path, sample_size):\n",
        "  lines = io.open(path, encoding = 'UTF-8').read().strip().split(\"\\n\")\n",
        "  words_pair = [[preprocess_text(t) for t in line.split(\"\\t\")] for line in lines[:sample_size]]\n",
        "  return zip(*words_pair)"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--01oNVVJ2uk"
      },
      "source": [
        "#Applying the function\n",
        "sample_size = 50000"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSiMh7VHKQgw"
      },
      "source": [
        "#Load the sample of size 50000\n",
        "eng_text, spa_text = data_creator(file_path,sample_size )"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGKQ1-rxKqx1",
        "outputId": "b2396614-b929-4ef1-8559-ffcb81d8b03c"
      },
      "source": [
        "print(f\"English text is : {eng_text[-1]}\")\n",
        "print()\n",
        "print(f\"Spanish text is: {spa_text[-1]}\")"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English text is : <start>the people are so friendly .<end>\n",
            "\n",
            "Spanish text is: <start>la gente es muy amable .<end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAo94cJhK9-m"
      },
      "source": [
        "#Tokenize and padding the data to the right"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAziKOkpMiu-"
      },
      "source": [
        "def ln_tokenization(text):\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = '')\n",
        "  tokenizer.fit_on_texts(text)\n",
        "  tensor = tokenizer.texts_to_sequences(text)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
        "  return tensor, tokenizer"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTIrIFhRN5-e"
      },
      "source": [
        "#Load the cleaned and prepared dataset for training our MT-model"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV1YtRAxODXU"
      },
      "source": [
        "def load_clean_data(path, sample_size = None):\n",
        "  target_lang, input_lang = data_creator(file_path, sample_size)\n",
        "  input_tensor, input_tokenizer = ln_tokenization(input_lang)\n",
        "  target_tensor, target_tokenizer = ln_tokenization(target_lang)\n",
        "  return input_tensor, target_tensor, input_tokenizer, target_tokenizer"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZD-McH6PQPY"
      },
      "source": [
        "sample_size = 50000\n",
        "input_tensor, target_tensor, input_lang, target_lang = load_clean_data(file_path, sample_size)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k_Ygw_tPs71",
        "outputId": "cd75b9c0-88ab-49b7-ab00-e62acab250f0"
      },
      "source": [
        "print(f\"Max_len_input: {input_tensor.shape[1]}, Max_len_output: {target_tensor.shape[1]}\")"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max_len_input: 14, Max_len_output: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvMtRFsYRCoX"
      },
      "source": [
        "#Split the data for training and testing \n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmYqwpZARd_D"
      },
      "source": [
        "x_train, x_test, y_train,y_test = train_test_split(input_tensor, target_tensor, test_size = 0.2)"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqJO7IR7SAzp",
        "outputId": "5b8fb0f9-33a9-4aa3-e358-77a6ae133411"
      },
      "source": [
        "print(f\"x_train_shape: {x_train.shape}, y_train_shape: {y_train.shape}\\nx_test_shape: {x_test.shape}, y_test_shape: {y_test.shape}\")"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_shape: (40000, 14), y_train_shape: (40000, 10)\n",
            "x_test_shape: (10000, 14), y_test_shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-dBHEP3S5vv"
      },
      "source": [
        "#Map every word in a text to an index (number)\n",
        "def create_index(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t != 0:\n",
        "      print(\"%d---->%s\" %(t, lang.index_word[t]))"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQpwCdsgTTqB",
        "outputId": "746c5658-b5f4-43eb-dddd-25da53ee2599"
      },
      "source": [
        "#Testing the map function\n",
        "print('Input language, index-word mapping')\n",
        "create_index(input_lang, x_train[10])"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input language, index-word mapping\n",
            "154----><start>las\n",
            "270---->cosas\n",
            "59---->son\n",
            "5369---->diferentes\n",
            "96---->ahora\n",
            "1---->.<end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98i8givrUBj4",
        "outputId": "3bb3fb1c-5734-4a27-f436-0abeb5f64809"
      },
      "source": [
        "print(\"output language, index-word\")\n",
        "create_index(target_lang, y_train[10])"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output language, index-word\n",
            "1087----><start>things\n",
            "27---->are\n",
            "1095---->different\n",
            "77---->now\n",
            "1---->.<end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2COumFQzUdD5"
      },
      "source": [
        "#Creating a tensorflow data type for easy training"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAzfHCGyU9_1"
      },
      "source": [
        "batch_size = 64\n",
        "BUFFER = len(x_train)\n",
        "step_per_epoch = BUFFER // batch_size\n",
        "units = 1024\n",
        "embedding_dim = 512\n",
        "input_voc_size = len(input_lang.word_index) + 1\n",
        "output_voc_size = len(target_lang.word_index) + 1\n",
        "\n"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKz-5pBfWWRc"
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER)\n",
        "train_data = train_data.batch(batch_size, drop_remainder = True)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_data = test_data.batch(batch_size, drop_remainder = True)"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg8TciqVXScR"
      },
      "source": [
        "#The above datasets are ready for training:\n",
        "#We employ model subclassing to build both encoder and decoder network\n",
        "#We the use layer subclassing to construct an attention mechanism\n",
        "#In this project we will employ additive attention (Bhanadau's attention) with 3 parameters to be learnt"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hqjHDNzdkO6"
      },
      "source": [
        "#The encoder network"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yFaospKdmvc"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, voc_size, batch_size, enc_units,embedding_dim, name = 'encoder', **kwargs):\n",
        "    super(Encoder, self).__init__(name = name, **kwargs)\n",
        "    self.batch_size = batch_size\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding_layer = tf.keras.layers.Embedding(input_dim = voc_size, \n",
        "                                                     output_dim = embedding_dim, \n",
        "                                                     name = 'embd_layer')\n",
        "    self.gru_layer = tf.keras.layers.GRU(units = self.enc_units, \n",
        "                                         return_sequences = True, \n",
        "                                         return_state = True, \n",
        "                                         kernel_initializer = 'glorot_uniform',\n",
        "                                         name = 'gru_layer',\n",
        "                                         dropout = 0.5, recurrent_dropout = 0.25)\n",
        "  def call(self, inputs, hidden):\n",
        "    inputs = self.embedding_layer(inputs)\n",
        "    dec_out, hidden_state = self.gru_layer(inputs, initial_state = hidden)\n",
        "    return dec_out, hidden_state\n",
        "    \n",
        "  def hidden_initializer(self):\n",
        "    return tf.zeros(shape = (self.batch_size, self.enc_units))\n",
        "           \n"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw3QvqO-g1pY"
      },
      "source": [
        "#Instantiate the class:"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FnAo97hg4qe",
        "outputId": "feaec539-9d45-4099-9095-67cacbbd1325"
      },
      "source": [
        "encoder = Encoder(input_voc_size, batch_size,units, embedding_dim, name = 'encoder')"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_layer will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CirgPmnChNJj"
      },
      "source": [
        "hidden = encoder.hidden_initializer()"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PF0wezQhi5u",
        "outputId": "fca96e82-99ab-4db9-9f19-a7810d0ca633"
      },
      "source": [
        "#Testing on the input data\n",
        "train_input_sample_batch, train_output_sample_batch = next(iter(train_data))\n",
        "train_input_sample_batch.shape, train_output_sample_batch.shape"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 14]), TensorShape([64, 10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIH7l1IYkUbn"
      },
      "source": [
        "enc_sample_out, enc_hidden = encoder(train_input_sample_batch, hidden)"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4VUCEubk8xu",
        "outputId": "fb938dae-0a29-4fae-bb92-7d6eb642b953"
      },
      "source": [
        "enc_sample_out.shape, enc_hidden.shape"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 14, 1024]), TensorShape([64, 1024]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz7OClVvlchQ",
        "outputId": "d33699bd-4eb9-4b73-f8d3-8acca55e50f6"
      },
      "source": [
        "print(\"I am tired: To be continued tomorrow!!!Thanks Jesus my Lord and my God\")"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am tired: To be continued tomorrow!!!Thanks Jesus my Lord and my God\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC2FpxDhl7-e"
      },
      "source": [
        "#The additive attention mechanism (Bhanadau's attention)\n",
        "#We use layer subclassing to construct attention network"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Yd2EKYshMs"
      },
      "source": [
        "class BhanadauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, name = 'attention', **kwargs):\n",
        "    super(BhanadauAttention, self).__init__(name = name)\n",
        "    self.W1 = tf.keras.layers.Dense(units = units,\n",
        "                                    kernel_initializer = 'random_normal', \n",
        "                                    activation = 'relu',\n",
        "                                    name = 'w1')\n",
        "    self.W2 = tf.keras.layers.Dense(units = units,\n",
        "                                    kernel_initializer = 'random_normal',\n",
        "                                    activation = 'relu',\n",
        "                                    name = 'w1')\n",
        "    self.V = tf.keras.layers.Dense(units = 1, \n",
        "                                   kernel_initializer = 'random_normal',\n",
        "                                   activation = 'relu',\n",
        "                                   name = 'v')\n",
        "  \n",
        "  def call(self, query, value):\n",
        "    query_expanded = tf.expand_dims(query, 1)# Add the time axis\n",
        "    score = self.V(tf.nn.tanh(self.W1(query_expanded)+ self.W2(value)))\n",
        "    attention_wt = tf.nn.softmax(score, axis = 1)\n",
        "    context = attention_wt * value\n",
        "    context_vector = tf.reduce_sum(context, axis = 1)\n",
        "    return context_vector, attention_wt"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X4MB_yrv_Z7"
      },
      "source": [
        "#Instantiste the class\n",
        "attention = BhanadauAttention(units = 10, name = 'attention')"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO1lX7MrwLma"
      },
      "source": [
        "#Testing on the sample batch\n",
        "sample_context_vector, sample_attention_wt = attention(enc_hidden, enc_sample_out)"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9npiSkAweg_",
        "outputId": "ad92acd7-63c4-4725-fcd5-3f8807f26291"
      },
      "source": [
        "sample_context_vector.shape, sample_attention_wt.shape"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 1024]), TensorShape([64, 14, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX57moJUxCEa"
      },
      "source": [
        "#The decoder's network.\n",
        "#We again use the model subclassing to build this network"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc7OTXKnx1Qf"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, voc_size, embedding_dim, dec_units,batch_size, name = 'decoder', **kwargs):\n",
        "    super(Decoder,self).__init__(name = name, **kwargs)\n",
        "    self.batch_size = batch_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding_layer = tf.keras.layers.Embedding(input_dim = voc_size, \n",
        "                                                     output_dim = embedding_dim,\n",
        "                                                     name = 'dec_embedding')\n",
        "    self.gru = tf.keras.layers.GRU(units = self.dec_units, \n",
        "                                   return_state = True,\n",
        "                                   return_sequences = True,\n",
        "                                   kernel_initializer = 'glorot_uniform',\n",
        "                                   recurrent_dropout = 0.25,\n",
        "                                   dropout = 0.5)\n",
        "    self.fc = tf.keras.layers.Dense(units = voc_size, activation = 'softmax', name = 'decoder_out')\n",
        "    self.attention = BhanadauAttention(self.dec_units)\n",
        "  \n",
        "  def call(self, inputs, hidden, enc_out):\n",
        "    enc_context_vec, attention_wt = self.attention(hidden,enc_out)\n",
        "    inputs = self.embedding_layer(inputs)\n",
        "    inputs = tf.concat([tf.expand_dims(enc_context_vec,1), inputs], axis = -1)\n",
        "    dec_out, hidden_state = self.gru(inputs)\n",
        "    dec_out = tf.reshape(dec_out, (-1, dec_out.shape[2]))\n",
        "    inputs = self.fc(dec_out)\n",
        "    return inputs, hidden_state, attention_wt\n"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WFowFbm3AtM",
        "outputId": "b8656e7d-54ff-4166-99da-7aad579d311c"
      },
      "source": [
        "#Instantiate the class\n",
        "decoder = Decoder(output_voc_size, embedding_dim,units,batch_size, name = 'decoder')"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzQSmkR93t9z"
      },
      "source": [
        "#Testing on a sample batch\n",
        "dec_out, dec_hidden, attention_wt = decoder(tf.random.uniform(shape = (batch_size, 1)), enc_hidden, enc_sample_out)"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCYJAT325JE3",
        "outputId": "b356027e-5d0b-42d9-f908-358a1a5f4426"
      },
      "source": [
        "print(f\"{dec_out.shape, dec_hidden.shape, attention_wt.shape}\")"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorShape([64, 7525]), TensorShape([64, 1024]), TensorShape([64, 14, 1]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOeVllwz5Y4l"
      },
      "source": [
        "#The training step"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJM4rf2DATFD"
      },
      "source": [
        "#We will train this model from scratch.\n"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGH0aAt8Ah4t"
      },
      "source": [
        "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate = 1e-3)\n"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my0vIigIBJDP"
      },
      "source": [
        "#The loss function"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ev2an5EBR4v"
      },
      "source": [
        "def my_loss(y_real, y_pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(y_real, 0)) #Return boolean for the mask\n",
        "  loss_ =loss_obj(y_real, y_pred)\n",
        "  mask = tf.cast(mask, dtype = loss_.dtype)\n",
        "  loss_*= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC8L_9tyCrj0"
      },
      "source": [
        "#Create a checkpoint to save the model periodically"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMBM61cKCz6J"
      },
      "source": [
        "chekpoint_dir = \"./training_checkpoints\"\n",
        "checkpoint_prefix = os.path.join(chekpoint_dir, 'chk')\n",
        "checkpoint = tf.train.Checkpoint(optimizer = optimizer, encoder = encoder, decoder = decoder)"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZgddxV1C1bF"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inputs, target, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_out, enc_hidden = encoder(inputs, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_lang.word_index['start']] * batch_size,1)\n",
        "    #Feeding the target as an input to predict the next output (Teacher forcing!!!)\n",
        "    for t in range(1, target.shape[1]):\n",
        "      predictions, hidden, _ = decoder(dec_input, dec_hidden,enc_out)\n",
        "      loss+=my_loss(target[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(target[:, t], 1)\n",
        "  batch_loss = (loss/int(target.shape[1]))\n",
        "  vars = encoder.trainable_variables + decoder.trainable_variables\n",
        "  grads = tape.gradient(loss, vars)\n",
        "  optimizer.apply_gradients(zip(grads, vars))\n",
        "  return batch_loss\n",
        "\n"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlyLYGCCNy0V"
      },
      "source": [
        "#Iterate over the epochs"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVYf2axQN89f",
        "outputId": "878bf707-3e81-44c2-972b-f670fbf979c0"
      },
      "source": [
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "  tic = time.time()\n",
        "  enc_hidden = encoder.hidden_initializer()\n",
        "  total_loss = 0\n",
        "  for (step,(inp, targ)) in enumerate(train_data.take(step_per_epoch)):\n",
        "    batch_loss = train_step(inp,targ, enc_hidden)\n",
        "    total_loss+=batch_loss\n",
        "    if step % 100 == 0:\n",
        "      print(f\"Epoch: {epoch + 1} Batch: {step} Loss: {batch_loss:.4f}\")\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "  print(f\"Epoch: {epoch + 1} Loss: {(total_loss/step_per_epoch):.4f}\")\n",
        "  toc = time.time()\n",
        "  print(f\"time elapse is : {time_fmt(toc - tic)}\")"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Batch: 0 Loss: 4.2539\n",
            "Epoch: 1 Batch: 100 Loss: 2.2254\n",
            "Epoch: 1 Batch: 200 Loss: 2.0658\n",
            "Epoch: 1 Batch: 300 Loss: 1.8565\n",
            "Epoch: 1 Batch: 400 Loss: 1.8110\n",
            "Epoch: 1 Batch: 500 Loss: 1.8295\n",
            "Epoch: 1 Batch: 600 Loss: 1.6503\n",
            "Epoch: 1 Loss: 1.9835\n",
            "time elapse is : 0: 001: 12.00\n",
            "Epoch: 2 Batch: 0 Loss: 1.6369\n",
            "Epoch: 2 Batch: 100 Loss: 1.4871\n",
            "Epoch: 2 Batch: 200 Loss: 1.3881\n",
            "Epoch: 2 Batch: 300 Loss: 1.5761\n",
            "Epoch: 2 Batch: 400 Loss: 1.4179\n",
            "Epoch: 2 Batch: 500 Loss: 1.2291\n",
            "Epoch: 2 Batch: 600 Loss: 1.3349\n",
            "Epoch: 2 Loss: 1.4379\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 3 Batch: 0 Loss: 1.1636\n",
            "Epoch: 3 Batch: 100 Loss: 1.3489\n",
            "Epoch: 3 Batch: 200 Loss: 1.1220\n",
            "Epoch: 3 Batch: 300 Loss: 1.1023\n",
            "Epoch: 3 Batch: 400 Loss: 1.1760\n",
            "Epoch: 3 Batch: 500 Loss: 1.0659\n",
            "Epoch: 3 Batch: 600 Loss: 1.1727\n",
            "Epoch: 3 Loss: 1.1495\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 4 Batch: 0 Loss: 0.9119\n",
            "Epoch: 4 Batch: 100 Loss: 1.1335\n",
            "Epoch: 4 Batch: 200 Loss: 0.9967\n",
            "Epoch: 4 Batch: 300 Loss: 0.8118\n",
            "Epoch: 4 Batch: 400 Loss: 0.9159\n",
            "Epoch: 4 Batch: 500 Loss: 0.8848\n",
            "Epoch: 4 Batch: 600 Loss: 0.9528\n",
            "Epoch: 4 Loss: 0.9361\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 5 Batch: 0 Loss: 0.7326\n",
            "Epoch: 5 Batch: 100 Loss: 0.8015\n",
            "Epoch: 5 Batch: 200 Loss: 0.7226\n",
            "Epoch: 5 Batch: 300 Loss: 0.7349\n",
            "Epoch: 5 Batch: 400 Loss: 0.7478\n",
            "Epoch: 5 Batch: 500 Loss: 0.7322\n",
            "Epoch: 5 Batch: 600 Loss: 0.7423\n",
            "Epoch: 5 Loss: 0.7756\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 6 Batch: 0 Loss: 0.7007\n",
            "Epoch: 6 Batch: 100 Loss: 0.7124\n",
            "Epoch: 6 Batch: 200 Loss: 0.6895\n",
            "Epoch: 6 Batch: 300 Loss: 0.7763\n",
            "Epoch: 6 Batch: 400 Loss: 0.5236\n",
            "Epoch: 6 Batch: 500 Loss: 0.6648\n",
            "Epoch: 6 Batch: 600 Loss: 0.6400\n",
            "Epoch: 6 Loss: 0.6474\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 7 Batch: 0 Loss: 0.4869\n",
            "Epoch: 7 Batch: 100 Loss: 0.4815\n",
            "Epoch: 7 Batch: 200 Loss: 0.4813\n",
            "Epoch: 7 Batch: 300 Loss: 0.5252\n",
            "Epoch: 7 Batch: 400 Loss: 0.6356\n",
            "Epoch: 7 Batch: 500 Loss: 0.5301\n",
            "Epoch: 7 Batch: 600 Loss: 0.4831\n",
            "Epoch: 7 Loss: 0.5463\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 8 Batch: 0 Loss: 0.4169\n",
            "Epoch: 8 Batch: 100 Loss: 0.5789\n",
            "Epoch: 8 Batch: 200 Loss: 0.3494\n",
            "Epoch: 8 Batch: 300 Loss: 0.5269\n",
            "Epoch: 8 Batch: 400 Loss: 0.4347\n",
            "Epoch: 8 Batch: 500 Loss: 0.4716\n",
            "Epoch: 8 Batch: 600 Loss: 0.6641\n",
            "Epoch: 8 Loss: 0.4700\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 9 Batch: 0 Loss: 0.2970\n",
            "Epoch: 9 Batch: 100 Loss: 0.2974\n",
            "Epoch: 9 Batch: 200 Loss: 0.4046\n",
            "Epoch: 9 Batch: 300 Loss: 0.4225\n",
            "Epoch: 9 Batch: 400 Loss: 0.5042\n",
            "Epoch: 9 Batch: 500 Loss: 0.4416\n",
            "Epoch: 9 Batch: 600 Loss: 0.5042\n",
            "Epoch: 9 Loss: 0.4148\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 10 Batch: 0 Loss: 0.3474\n",
            "Epoch: 10 Batch: 100 Loss: 0.3793\n",
            "Epoch: 10 Batch: 200 Loss: 0.4158\n",
            "Epoch: 10 Batch: 300 Loss: 0.3519\n",
            "Epoch: 10 Batch: 400 Loss: 0.3916\n",
            "Epoch: 10 Batch: 500 Loss: 0.4233\n",
            "Epoch: 10 Batch: 600 Loss: 0.5037\n",
            "Epoch: 10 Loss: 0.3784\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 11 Batch: 0 Loss: 0.2892\n",
            "Epoch: 11 Batch: 100 Loss: 0.3468\n",
            "Epoch: 11 Batch: 200 Loss: 0.3681\n",
            "Epoch: 11 Batch: 400 Loss: 0.4529\n",
            "Epoch: 11 Batch: 500 Loss: 0.3510\n",
            "Epoch: 11 Batch: 600 Loss: 0.4792\n",
            "Epoch: 11 Loss: 0.3536\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 12 Batch: 0 Loss: 0.2673\n",
            "Epoch: 12 Batch: 100 Loss: 0.2608\n",
            "Epoch: 12 Batch: 200 Loss: 0.3136\n",
            "Epoch: 12 Batch: 300 Loss: 0.3449\n",
            "Epoch: 12 Batch: 400 Loss: 0.3744\n",
            "Epoch: 12 Batch: 500 Loss: 0.2920\n",
            "Epoch: 12 Batch: 600 Loss: 0.3815\n",
            "Epoch: 12 Loss: 0.3304\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 13 Batch: 0 Loss: 0.2650\n",
            "Epoch: 13 Batch: 100 Loss: 0.3063\n",
            "Epoch: 13 Batch: 200 Loss: 0.2446\n",
            "Epoch: 13 Batch: 300 Loss: 0.3280\n",
            "Epoch: 13 Batch: 400 Loss: 0.4028\n",
            "Epoch: 13 Batch: 500 Loss: 0.3681\n",
            "Epoch: 13 Batch: 600 Loss: 0.2457\n",
            "Epoch: 13 Loss: 0.3210\n",
            "time elapse is : 0: 000: 58.00\n",
            "Epoch: 14 Batch: 0 Loss: 0.2186\n",
            "Epoch: 14 Batch: 100 Loss: 0.3356\n",
            "Epoch: 14 Batch: 200 Loss: 0.2955\n",
            "Epoch: 14 Batch: 300 Loss: 0.2769\n",
            "Epoch: 14 Batch: 400 Loss: 0.3353\n",
            "Epoch: 14 Batch: 500 Loss: 0.4552\n",
            "Epoch: 14 Batch: 600 Loss: 0.4167\n",
            "Epoch: 14 Loss: 0.3180\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 15 Batch: 0 Loss: 0.3194\n",
            "Epoch: 15 Batch: 100 Loss: 0.2663\n",
            "Epoch: 15 Batch: 200 Loss: 0.3727\n",
            "Epoch: 15 Batch: 300 Loss: 0.4473\n",
            "Epoch: 15 Batch: 400 Loss: 0.3324\n",
            "Epoch: 15 Batch: 500 Loss: 0.3696\n",
            "Epoch: 15 Batch: 600 Loss: 0.3183\n",
            "Epoch: 15 Loss: 0.3153\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 16 Batch: 0 Loss: 0.1607\n",
            "Epoch: 16 Batch: 100 Loss: 0.2415\n",
            "Epoch: 16 Batch: 200 Loss: 0.3225\n",
            "Epoch: 16 Batch: 300 Loss: 0.3967\n",
            "Epoch: 16 Batch: 400 Loss: 0.2573\n",
            "Epoch: 16 Batch: 500 Loss: 0.4177\n",
            "Epoch: 16 Batch: 600 Loss: 0.3568\n",
            "Epoch: 16 Loss: 0.3082\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 17 Batch: 0 Loss: 0.2714\n",
            "Epoch: 17 Batch: 100 Loss: 0.2152\n",
            "Epoch: 17 Batch: 200 Loss: 0.2577\n",
            "Epoch: 17 Batch: 300 Loss: 0.3489\n",
            "Epoch: 17 Batch: 400 Loss: 0.3099\n",
            "Epoch: 17 Batch: 500 Loss: 0.3670\n",
            "Epoch: 17 Batch: 600 Loss: 0.3078\n",
            "Epoch: 17 Loss: 0.2928\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 18 Batch: 0 Loss: 0.2633\n",
            "Epoch: 18 Batch: 100 Loss: 0.2340\n",
            "Epoch: 18 Batch: 200 Loss: 0.2753\n",
            "Epoch: 18 Batch: 300 Loss: 0.3168\n",
            "Epoch: 18 Batch: 400 Loss: 0.3362\n",
            "Epoch: 18 Batch: 500 Loss: 0.2428\n",
            "Epoch: 18 Batch: 600 Loss: 0.2999\n",
            "Epoch: 18 Loss: 0.2915\n",
            "time elapse is : 0: 000: 59.00\n",
            "Epoch: 19 Batch: 0 Loss: 0.2303\n",
            "Epoch: 19 Batch: 100 Loss: 0.1907\n",
            "Epoch: 19 Batch: 200 Loss: 0.2798\n",
            "Epoch: 19 Batch: 300 Loss: 0.3217\n",
            "Epoch: 19 Batch: 400 Loss: 0.3363\n",
            "Epoch: 19 Batch: 500 Loss: 0.3112\n",
            "Epoch: 19 Batch: 600 Loss: 0.3685\n",
            "Epoch: 19 Loss: 0.2905\n",
            "time elapse is : 0: 000: 58.00\n",
            "Epoch: 20 Batch: 0 Loss: 0.2140\n",
            "Epoch: 20 Batch: 100 Loss: 0.2883\n",
            "Epoch: 20 Batch: 200 Loss: 0.3032\n",
            "Epoch: 20 Batch: 300 Loss: 0.2974\n",
            "Epoch: 20 Batch: 400 Loss: 0.2708\n",
            "Epoch: 20 Batch: 500 Loss: 0.2448\n",
            "Epoch: 20 Batch: 600 Loss: 0.3692\n",
            "Epoch: 20 Loss: 0.2907\n",
            "time elapse is : 0: 000: 59.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfTvWLN7Redj"
      },
      "source": [
        ""
      ],
      "execution_count": 199,
      "outputs": []
    }
  ]
}